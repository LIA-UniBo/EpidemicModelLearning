{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import covasim as cv\n",
    "from pyDOE import lhs\n",
    "from src.data import get_regional_data\n",
    "from src.interventions import get_sampling_interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Initial Configuration\n",
    "\n",
    "Once the dataset is calibrated, we would like to use it to predict possible future evolutions of the pandemic.\n",
    "\n",
    "Given that *Covasim* needs a warm-up period to reach a stabler pandemic condition, we will proceed by generating the dataset in this way:\n",
    "* we run a simulation for a total of *zones_starting_day $+$ num_zones $\\cdot$ time_interval days*\n",
    "* we discard the first *zones_starting_day* days (251, in our case), meaning that we will collect data from 01/11/2020 on, which is when the evolution of the pandemic started having a behaviour more similar to the one we have right now (i.e., after the summer period)\n",
    "* we aggregate the data on a time window of size *2 $\\cdot$ time_interval* (6 weeks, in our case), in which we consider the first half of the data as \"inputs\", and the last half as \"outputs\"\n",
    "  * right in the middle of these windows there is a zone change\n",
    "  * it will be used as input of the surrogate model together with the other data\n",
    "\n",
    "In this way, we aim at maximizing the number of samples that we can get from each run, while trying at the same time to cover the data space in a (hopefully) almost uniform way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_zones = 8\n",
    "time_interval = 21\n",
    "zones_starting_day = 251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now retrieve the parameters' configuration that we obtained from the calibration step and set the length of the simulation from the previously set values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open('../res/parameters.json', 'r') as json_file:\n",
    "    j = json.load(json_file)\n",
    "\n",
    "intervention_params = j['intervention_params']\n",
    "initial_params = j['initial_params']\n",
    "initial_params['n_days'] = zones_starting_day + num_zones * time_interval\n",
    "df = get_regional_data(4.46e6 / initial_params['pop_size'])\n",
    "\n",
    "initial_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latin-Hypercube Sampling\n",
    "\n",
    "We have *4* possible categories for the zones, and each run of the simulator will consider a sequence of *8* zone changes. Thus, the total number of possible sequences is $4^8 = 65536$, which, if we consider an average of 2 minutes for each run, will end the generation phase after 90 days.\n",
    "\n",
    "This is clearly not desirable, thus we rely on *Latin-Hypercube Sampling* to generate a subset of *300* sequences which, however, is supposed to cover the sequences space almost uniformly. Again, if we consider an average of 2 minutes for each run, will end the generation phase after 10 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = lhs(n=num_zones, samples=300)\n",
    "samples = pd.DataFrame(samples)\n",
    "for i in range(num_zones):\n",
    "    samples[i] = samples[i].map(lambda v: int(4 * v)).map({0: 'W', 1: 'Y', 2: 'O', 3: 'R'})\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can start the generation script, from which we will collect the data about *hospitalized* individuals and cumulative *diagnosed* cases and *deaths*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for idx, zones in samples.iterrows():\n",
    "    print(f'Generating samples for simulation {idx + 1:0{len(str(samples.shape[0]))}}/{samples.shape[0]}', end='')\n",
    "    start_time = time.time()\n",
    "    intervs = get_sampling_interventions(zones, intervention_params, time_interval)\n",
    "    sim = cv.Sim(pars={**initial_params, 'rand_seed': idx}, interventions=intervs, datafile=df)\n",
    "    sim.run()\n",
    "    # concatenate data in an array of shape (num_days, 4)\n",
    "    result = np.concatenate((\n",
    "        sim.results['n_severe'].values + sim.results['n_critical'].values,\n",
    "        sim.results['cum_diagnoses'].values,\n",
    "        sim.results['cum_deaths'].values\n",
    "    )).reshape(3, -1).transpose()\n",
    "    # retrieve data about last num_zones * time_interval and flatten it to get num_zones rows\n",
    "    result = result[-num_zones * time_interval:].reshape(num_zones, -1)\n",
    "    # get data about two subsequent zones with respective zone colors\n",
    "    data += [np.concatenate((inp, out, zones[z:z + 2])) for z, (inp, out), in enumerate(zip(result[:-1], result[1:]))]\n",
    "    print(f' -- elapsed time: {time.time() - start_time:.4}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Saving the Dataset\n",
    "\n",
    "We can now collect the data into a pandas dataframe and store it in a *csv* file to be used to train the surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 2 * time_interval + 2\n",
    "\n",
    "columns = [f'{c}_{d}' for d in range(0, 2 * time_interval) for c in ['hosp', 'diag', 'dead']]\n",
    "data = pd.DataFrame(data, columns=columns + ['init_zone', 'actuated_zone'])\n",
    "data.to_csv('../res/dataset.csv', index=False)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}